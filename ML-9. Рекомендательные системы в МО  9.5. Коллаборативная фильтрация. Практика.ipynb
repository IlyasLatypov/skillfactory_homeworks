{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /home/aprosvetov/netflix/combined_data_1.txt does not exist: '/home/aprosvetov/netflix/combined_data_1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b79a08e23ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/aprosvetov/netflix/combined_data_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Cust_Id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /home/aprosvetov/netflix/combined_data_1.txt does not exist: '/home/aprosvetov/netflix/combined_data_1.txt'"
     ]
    }
   ],
   "source": [
    "data_all = []\n",
    "for i in range(4):\n",
    "    data = pd.read_csv('/home/aprosvetov/netflix/combined_data_' + str(i+1)+'.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n",
    "    data_all.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (100498277, 2)\n",
      "-Dataset examples-\n",
      "           Cust_Id  Rating\n",
      "0               1:     NaN\n",
      "5000000    2560324     4.0\n",
      "10000000   2271935     2.0\n",
      "15000000   1921803     2.0\n",
      "20000000   1933327     3.0\n",
      "25000000   1465002     3.0\n",
      "30000000    961023     4.0\n",
      "35000000   1372532     5.0\n",
      "40000000    854274     5.0\n",
      "45000000    116334     3.0\n",
      "50000000    768483     3.0\n",
      "55000000   1331144     5.0\n",
      "60000000   1609324     2.0\n",
      "65000000   1699240     3.0\n",
      "70000000   1776418     4.0\n",
      "75000000   1643826     5.0\n",
      "80000000    932047     4.0\n",
      "85000000   2292868     4.0\n",
      "90000000    932191     4.0\n",
      "95000000   1815101     3.0\n",
      "100000000   872339     4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df.index = np.arange(0,len(df))\n",
    "print('Full dataset shape: {}'.format(df.shape))\n",
    "print('-Dataset examples-')\n",
    "print(df.iloc[::5000000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_nan = pd.DataFrame(pd.isnull(df.Rating))\n",
    "df_nan = df_nan[df_nan['Rating'] == True]\n",
    "df_nan = df_nan.reset_index()\n",
    "\n",
    "movie_np = []\n",
    "movie_id = 1\n",
    "\n",
    "for i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n",
    "    # numpy approach\n",
    "    temp = np.full((1,np.abs(i-j-1)), movie_id)\n",
    "    movie_np = np.append(movie_np, temp)\n",
    "    movie_id += 1\n",
    "\n",
    "# Account for last record and corresponding length\n",
    "# numpy approach\n",
    "last_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\n",
    "movie_np = np.append(movie_np, last_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['Rating'])]\n",
    "\n",
    "df['Movie_Id'] = movie_np.astype(int)\n",
    "df['Cust_Id'] = df['Cust_Id'].astype(int)\n",
    "print('-Dataset examples-')\n",
    "print(df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/srv/aprosvetov/netflix/data_prep.csv', sep=';', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/srv/aprosvetov/netflix/data_prep.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17770"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Movie_Id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480189"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Cust_Id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим работать с датасетом Netflix.\n",
    "\n",
    "Возьмём подвыборку из 10000 случайных кастомеров и 5000 фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sample = df.Cust_Id.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_sample = df.Movie_Id.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации простых рекомендаций с помощью коллаборативной фильтрации можно воспользоваться модулем surprise. Загрузим в модуль surprise наш датасет с помощью метода Reader. \n",
    "\n",
    "Предварительно необходимо установить модуль surprise, он не является предустановленным.\n",
    "\n",
    "Это можно сделать через pip. В случае, если это не работает, можно воспользоваться одним из следующих четырёх вариантов:\n",
    "\n",
    "conda install -c conda-forge scikit-surprise\n",
    "conda install -c conda-forge/label/gcc7 scikit-surprise\n",
    "conda install -c conda-forge/label/cf201901 scikit-surprise\n",
    "conda install -c conda-forge/label/cf202003 scikit-surprise\n",
    "\n",
    "Итак, импортируем установленную библиотеку и приступаем к работе.\n",
    "\n",
    "На странице surprise указано, что для surprise также необходим установленный компилятор C. \n",
    "\n",
    "Для установки компилятора C можно установитьь MinGW (у меня стоит такой, surprise установился без ошибок). Вот инструкция по установке: https://o7planning.org/ru/10467/installing-c-cpp-compiler-mingw\n",
    "\n",
    "В инструкции, кажется, немного отличается внешний вид сайта на скриншотах, но не критично. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем только те оценки, которые относятся к выбранному подмножеству кастомеров и только те оценки, которые относятся к выбранному подмножеству фильмов. Именно в такой последовательности — сначала Cust_Id, затем Movie_Id, затем Rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df[df.Cust_Id.isin(cust_sample) &\n",
    "                              df.Movie_Id.isin(movie_sample)][['Cust_Id', 'Movie_Id', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модуле surprise есть несколько реализаций коллаборативной фильтрации. Мы возьмем одну из самых самых простых — принцип ближайших соседей.\n",
    "\n",
    "Принцип коллаборативной фильтрации заключается в следующем:\n",
    "\n",
    "Для каждого человека находится небольшое множество похожих на него зрителей с оценками примерно такими же, какие поставил человек на ряд фильмов (item). Из этой группы можно усреднить оценки на просмотренные фильмы и для тех членов группы, у которых ещё не было просмотров этих фильмов экстраполировать значения оценок в этих ячейках.\n",
    "\n",
    "Таким образом, у нас появляется некая средняя оценка в группе для каждого фильма из просмотренных, и мы можем предположить, что тем людям, которые ещё не успели посмотреть эти фильмы, они понравятся.\n",
    "\n",
    "Так как размерность по пользователям больше, чем размерность по фильмам, то выгоднее использовать не user-based алгоритм, а item-based. В этом случае вектор будет состоять не из оценок одного пользователя на различные фильмы, а будет содержать все оценки фильма от многих пользователей. Таким образом мы получим больший вектор, но само количество векторов будет меньше. А если меньше количество векторов, то проще посчитать матрицу из взаимной дистанции.\n",
    "\n",
    "Именно это мы задаем в качестве параметров алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': False\n",
    "}\n",
    " \n",
    "knn = KNNBasic(sim_options=sim_options)\n",
    "trainingSet = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем алгоритм и формирует датасет для тренировки специальной функцией build_full_trainset().\n",
    "\n",
    "После этого проводим тренировку модели на сформированном тренировочном датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x7fcf281fe160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(trainingSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = trainingSet.build_anti_testset()\n",
    "predictions = knn.test(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью натренированной модели мы можем проскорить остальные оценки. Для этого сгенерируем тестовый сет и построим предсказание по этому датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('movie_titles.csv', encoding = \"ISO-8859-1\", \n",
    "                     header = None, \n",
    "                     names = ['Movie_Id', 'Year', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid=2503129, iid=44, r_ui=3.5881227434677645, est=4.474997108906492, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid=2503129, iid=47, r_ui=3.5881227434677645, est=3.9245326082785117, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid=2503129, iid=76, r_ui=3.5881227434677645, est=4.424598056472339, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid=2503129, iid=83, r_ui=3.5881227434677645, est=4.374795599566862, details={'actual_k': 40, 'was_impossible': False})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат получился не удобочитаемым. Поэтому давайте сделаем вспомогательную функцию, которая будет брать топ-3 фильмов и их оценки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    " \n",
    "def get_top3_recommendations(predictions, topN = 3):\n",
    "     \n",
    "    top_recs = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_recs[uid].append((iid, est))\n",
    "     \n",
    "    for uid, user_ratings in top_recs.items():\n",
    "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
    "        top_recs[uid] = user_ratings[:topN]\n",
    "     \n",
    "    return top_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатываем наше предсказание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_recommendations = get_top3_recommendations(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью следующей функции переведем тексты фильмов в удобочитаемый вид, то есть раскодируем заглавия фильмов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def print_recs(i):\n",
    "    for (a, b) in top3_recommendations[i]:\n",
    "        print(titles[titles.Movie_Id == a]['Name'].values[0], np.round(b,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Blair Witch Project 4.65\n",
      "Lone Wolf and Cub: Sword of Vengeance 4.63\n",
      "The Basketball Diaries 4.63\n"
     ]
    }
   ],
   "source": [
    "i = 262149\n",
    "print_recs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью этой функции выведем рекомендации для случайного пользователя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family Guy: Vol. 1: Seasons 1-2 4.32\n",
      "Family Guy: Vol. 2: Season 3 4.32\n",
      "Team America: World Police 4.3\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(list(top3_recommendations.keys()))\n",
    "\n",
    "print_recs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что смотрел этот человек, и выберем из нашего датасета те фильмы, которые этот человек оценил на 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Deer Hunter', 'Dogma', 'Kill Bill: Vol. 2',\n",
       "       'The Matrix: Revolutions', 'Boyz N the Hood',\n",
       "       'Dragon: The Bruce Lee Story', 'The Sandlot', 'Man on Fire',\n",
       "       'Hook', 'Casino: 10th Anniversary Edition',\n",
       "       'Die Hard 2: Die Harder', \"Cheech & Chong's Up in Smoke\",\n",
       "       'Lord of the Rings: The Fellowship of the Ring', 'Braveheart',\n",
       "       'Elf', 'Half Baked', 'Lost: Season 1',\n",
       "       'Fear and Loathing in Las Vegas', 'The Matrix: Reloaded', 'Signs',\n",
       "       'Patch Adams', 'Army of Darkness',\n",
       "       'South Park: Passion of the Jew', 'Mission: Impossible',\n",
       "       'Tommy Boy', 'Jay and Silent Bob Strike Back',\n",
       "       'Jackass: The Movie',\n",
       "       'Star Wars: Episode V: The Empire Strikes Back',\n",
       "       'GoodFellas: Special Edition', 'Snatch', 'Fight Club',\n",
       "       'Scarface: 20th Anniversary Edition', 'Awakenings', 'Michael',\n",
       "       'The Cable Guy', 'Natural Born Killers',\n",
       "       'Lord of the Rings: The Two Towers: Extended Edition',\n",
       "       'The Lord of the Rings: The Fellowship of the Ring: Extended Edition',\n",
       "       'Blade', 'Super Troopers', 'All About the Benjamins',\n",
       "       'Murder in the First', 'The Evil Dead', 'Star Trek: Nemesis',\n",
       "       'Rounders', 'Good Will Hunting', 'Terminator 2: Extreme Edition',\n",
       "       'The Chronicles of Riddick', 'Friday',\n",
       "       'The Exorcist: Restored Version',\n",
       "       'Beavis and Butt-head Do America', 'Die Hard',\n",
       "       'Searching for Bobby Fischer', 'Akira', 'Waterworld',\n",
       "       'Pulp Fiction', 'American History X', 'Far and Away',\n",
       "       'Forrest Gump', 'Lord of the Rings: The Two Towers',\n",
       "       \"White Men Can't Jump\", 'The Boondock Saints', 'Office Space',\n",
       "       'Gladiator', \"Donnie Darko: Director's Cut\",\n",
       "       'Lord of the Rings: The Return of the King', 'The Patriot',\n",
       "       'The Shawshank Redemption: Special Edition', 'South Park: Bigger',\n",
       "       'Die Hard With a Vengeance', 'The Matrix', 'Beetlejuice',\n",
       "       'Dead Poets Society',\n",
       "       'Lord of the Rings: The Return of the King: Extended Edition',\n",
       "       'City Slickers', 'Zoolander', 'Band of Brothers',\n",
       "       'The Big Lebowski', 'The Running Man', 'Amadeus', 'Raw',\n",
       "       'The Sopranos: Season 1', 'Demolition Man',\n",
       "       'Star Wars: Episode IV: A New Hope', 'Planet of the Apes',\n",
       "       'The Green Mile', 'Blow', 'Tombstone', 'Donnie Darko',\n",
       "       'South Park: Winter Wonderland', 'Meet the Parents',\n",
       "       \"The Devil's Advocate\", 'Saving Private Ryan', 'Full Metal Jacket',\n",
       "       'Tomorrow Never Dies'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films = data.df[(data.df.Cust_Id == i) & (data.df.Rating == 5)]['Movie_Id'].values\n",
    "titles[titles.Movie_Id.isin(films)]['Name'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь самостоятельно построить коллаборативную фильтрацию для книг. Загрузим наши данные и приведем их в привычный вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user.columns = ['userID', 'Location', 'Age']\n",
    "rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "df = pd.merge(user, rating, on='userID', how='inner')\n",
    "df.drop(['Location', 'Age'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
